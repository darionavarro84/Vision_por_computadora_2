{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-08 09:51:04.498029: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import expand_dims\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defino función de predicción\n",
    "def prediccion(modelo,imagen,treshold):\n",
    "\n",
    "    # Reshape para que coincida con el formato de entrada del modelo\n",
    "\n",
    "     nueva_altura = 224\n",
    "     nuevo_ancho = 224\n",
    "\n",
    "     h, w = imagen.shape[:2]\n",
    "\n",
    "    # Calcular el nuevo tamaño manteniendo la relación de aspecto original\n",
    "     if h < w:\n",
    "          nueva_altura_temp = int(nueva_altura * (h / w))\n",
    "          nuevo_ancho_temp = nuevo_ancho\n",
    "     else:\n",
    "          nueva_altura_temp = nueva_altura\n",
    "          nuevo_ancho_temp = int(nuevo_ancho * (w / h))\n",
    "\n",
    "     imagen_redimensionada = cv2.resize(imagen,(nuevo_ancho_temp , nueva_altura_temp))\n",
    "\n",
    "     imagen_redimensionada = imagen_redimensionada / 255.0\n",
    "\n",
    "     # Crear una imagen en blanco del tamaño objetivo\n",
    "     imagen_final = np.zeros((nueva_altura, nuevo_ancho, 3), dtype=np.uint8)\n",
    "\n",
    "     # Calcular las coordenadas para copiar la imagen redimensionada en el centro\n",
    "     y_offset = (nueva_altura - nueva_altura_temp) // 2\n",
    "     x_offset = (nuevo_ancho - nuevo_ancho_temp) // 2\n",
    "\n",
    "     # Copiar la región redimensionada en la ubicación calculada\n",
    "     imagen_final[y_offset:y_offset + nueva_altura_temp, x_offset:x_offset + nuevo_ancho_temp] = imagen_redimensionada\n",
    "\n",
    "     imagen_final = np.expand_dims(imagen_final, axis=0)\n",
    "\n",
    "\n",
    "     # Realizar predicciones\n",
    "     prediccion_imagen = modelo.predict(imagen_final)\n",
    "\n",
    "     if prediccion_imagen > treshold:\n",
    "          print(prediccion_imagen)\n",
    "          return f\"La persona tiene sueño:[{prediccion_imagen}]\"\n",
    "     else:\n",
    "          print(prediccion_imagen)\n",
    "          return f\"La persona no tiene sueño:[{prediccion_imagen}]\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundBox:\n",
    "    def __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n",
    "        self.xmin = xmin\n",
    "        self.ymin = ymin\n",
    "        self.xmax = xmax\n",
    "        self.ymax = ymax\n",
    "        self.objness = objness\n",
    "        self.classes = classes\n",
    "        self.label = -1\n",
    "        self.score = -1\n",
    "        \n",
    "    def get_label(self):\n",
    "        if self.label == -1:\n",
    "            self.label = np.argmax(self.classes)\n",
    "        \n",
    "        return self.label\n",
    "    \n",
    "    def get_score(self):\n",
    "        if self.score == -1:\n",
    "            self.score = self.classes[self.get_label()]\n",
    "        \n",
    "        return self.score\n",
    "\n",
    "\n",
    "def _sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_netout(netout, anchors, obj_thresh, net_h, net_w):\n",
    "    grid_h, grid_w = netout.shape[:2] # 0 and 1 is row and column 13*13\n",
    "    nb_box = 3 # 3 anchor boxes\n",
    "    netout = netout.reshape((grid_h, grid_w, nb_box, -1)) #13*13*3 ,-1\n",
    "    nb_class = netout.shape[-1] - 5\n",
    "    boxes = []\n",
    "    netout[..., :2]  = _sigmoid(netout[..., :2])\n",
    "    netout[..., 4:]  = _sigmoid(netout[..., 4:])\n",
    "    netout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]\n",
    "    netout[..., 5:] *= netout[..., 5:] > obj_thresh\n",
    "    \n",
    "    for i in range(grid_h*grid_w):\n",
    "        row = i / grid_w\n",
    "        col = i % grid_w\n",
    "        for b in range(nb_box):\n",
    "            # 4th element is objectness score\n",
    "            objectness = netout[int(row)][int(col)][b][4]\n",
    "            if(objectness.all() <= obj_thresh): continue\n",
    "            # first 4 elements are x, y, w, and h\n",
    "            x, y, w, h = netout[int(row)][int(col)][b][:4]\n",
    "            x = (col + x) / grid_w # center position, unit: image width\n",
    "            y = (row + y) / grid_h # center position, unit: image height\n",
    "            w = anchors[2 * b + 0] * np.exp(w) / net_w # unit: image width\n",
    "            h = anchors[2 * b + 1] * np.exp(h) / net_h # unit: image height\n",
    "            # last elements are class probabilities\n",
    "            classes = netout[int(row)][col][b][5:]\n",
    "            box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)\n",
    "            boxes.append(box)\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):\n",
    "    new_w, new_h = net_w, net_h\n",
    "    for i in range(len(boxes)):\n",
    "        x_offset, x_scale = (net_w - new_w)/2./net_w, float(new_w)/net_w\n",
    "        y_offset, y_scale = (net_h - new_h)/2./net_h, float(new_h)/net_h\n",
    "        boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n",
    "        boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n",
    "        boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n",
    "        boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _interval_overlap(interval_a, interval_b):\n",
    "    x1, x2 = interval_a\n",
    "    x3, x4 = interval_b\n",
    "    if x3 < x1:\n",
    "        if x4 < x1:\n",
    "            return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x1\n",
    "    else:\n",
    "        if x2 < x3:\n",
    "            return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x3\n",
    "\n",
    "#intersection over union        \n",
    "def bbox_iou(box1, box2):\n",
    "    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n",
    "    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n",
    "    intersect = intersect_w * intersect_h\n",
    "    \n",
    "    \n",
    "    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin  \n",
    "    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n",
    "    \n",
    "    #Union(A,B) = A + B - Inter(A,B)\n",
    "    union = w1*h1 + w2*h2 - intersect\n",
    "    return float(intersect) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_nms(boxes, nms_thresh):    #boxes from correct_yolo_boxes and  decode_netout\n",
    "    if len(boxes) > 0:\n",
    "        nb_class = len(boxes[0].classes)\n",
    "    else:\n",
    "        return\n",
    "    for c in range(nb_class):\n",
    "        sorted_indices = np.argsort([-box.classes[c] for box in boxes])\n",
    "        for i in range(len(sorted_indices)):\n",
    "            index_i = sorted_indices[i]\n",
    "            if boxes[index_i].classes[c] == 0: continue\n",
    "            for j in range(i+1, len(sorted_indices)):\n",
    "                index_j = sorted_indices[j]\n",
    "                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n",
    "                    boxes[index_j].classes[c] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare an image\n",
    "def load_image_pixels(image, shape):\n",
    "    width, height = image.size\n",
    "    # load the image with the required size\n",
    "    image = load_img(filename, target_size=shape) # target_size argument to resize the image after loading\n",
    "    # convert to numpy array\n",
    "    image = img_to_array(image)\n",
    "    # scale pixel values to [0, 1]\n",
    "    image = image.astype('float32')\n",
    "    image /= 255.0  #rescale the pixel values from 0-255 to 0-1 32-bit floating point values.\n",
    "    # add a dimension so that we have one sample\n",
    "    image = expand_dims(image, 0)\n",
    "    return image, width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw all results\n",
    "#def draw_boxes(image, v_boxes, v_labels, v_scores):\n",
    "def draw_boxes(image, v_boxes, label):\n",
    "    #load the image\n",
    "    img = image\n",
    "    for i in range(len(v_boxes)):\n",
    "        # retrieving the coordinates from each bounding box\n",
    "        box = v_boxes[i]\n",
    "        # get coordinates\n",
    "        y1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax\n",
    "        start_point = (x1, y1) \n",
    "        # Ending coordinate\n",
    "        # represents the bottom right corner of rectangle \n",
    "        end_point = (x2, y2) \n",
    "        # Red color in BGR \n",
    "        color = (0, 0, 255) \n",
    "        # Line thickness of 2 px \n",
    "        thickness = 2\n",
    "        # font \n",
    "        font = cv2.FONT_HERSHEY_PLAIN \n",
    "        # fontScale \n",
    "        fontScale = 1.5\n",
    "        #create the shape\n",
    "        img = cv2.rectangle(img, start_point, end_point, color, thickness) \n",
    "        # draw text and score in top left corner\n",
    "        #label = \"%s (%.3f)\" % (v_labels[i], v_scores[i])\n",
    "        img = cv2.putText(img, label, (x1,y1), font,  \n",
    "                   fontScale, color, thickness, cv2.LINE_AA)\n",
    "    return img\n",
    "    # show the plot\n",
    "    #output = \"outputs/\"+filename.rsplit(\"/\")[1].rsplit(\".\")[0]+'_yolov3.jpg'\n",
    "    #save the image\n",
    "    #cv2.imwrite(output,img)\n",
    "    #cv2.imshow(\"yolov3\",img)\n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all of the results above a threshold\n",
    "def get_boxes(boxes, labels, thresh):\n",
    "    v_boxes, v_labels, v_scores = list(), list(), list()\n",
    "    # enumerate all boxes\n",
    "    for box in boxes:\n",
    "        # enumerate all possible labels\n",
    "        for i in range(len(labels)):\n",
    "            # check if the threshold for this label is high enough\n",
    "            if box.classes[i] > thresh:\n",
    "                v_boxes.append(box)\n",
    "                v_labels.append(labels[i])\n",
    "                v_scores.append(box.classes[i]*100)\n",
    "    \n",
    "    return v_boxes, v_labels, v_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_crop(img, v_boxes):\n",
    "    # Create a directory to save cropped images\n",
    "    #output_dir = \"outputs/\"\n",
    "    #os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for i, box in enumerate(v_boxes):\n",
    "        y1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax\n",
    "\n",
    "        # Crop the region inside the bounding box\n",
    "        cropped_region = img[y1:y2, x1:x2]\n",
    "        # Save the cropped region as a new image\n",
    "        #output_path = os.path.join(output_dir, f\"box_{i+1}.jpg\")\n",
    "        #cv2.imwrite(output_path, cropped_region)\n",
    "    return cropped_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFace(yhat,frame):\n",
    "    input_w, input_h = 416, 416\n",
    "    image_w, image_h = frame.shape[0],frame.shape[1]\n",
    "    # define the anchors\n",
    "    anchors = [[116,90, 156,198, 373,326], [30,61, 62,45, 59,119], [10,13, 16,30, 33,23]]  \n",
    "\n",
    "    # define the probability threshold for detected objects\n",
    "    class_threshold = 0.6\n",
    "    labels = [\"face\"]\n",
    "    boxes = list()\n",
    "    for i in range(len(yhat)):\n",
    "        # decode the output of the network\n",
    "        boxes += decode_netout(yhat[i][0], anchors[i], class_threshold, input_h, input_w)\n",
    "        \n",
    "    # correct the sizes of the bounding boxes for the shape of the image\n",
    "    correct_yolo_boxes(boxes, image_h, image_w, input_h, input_w)\n",
    "\n",
    "    # suppress non-maximal boxes\n",
    "    do_nms(boxes, 0.5)  #Discard all boxes with pc less or equal to 0.5\n",
    "\n",
    "    # get the details of the detected objects\n",
    "    v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)\n",
    "\n",
    "\n",
    "    # draw what we found\n",
    "    # draw_boxes(frame, v_boxes, v_labels, v_scores)\n",
    "    extractedFace = img_crop(frame,v_boxes)\n",
    "    return extractedFace,v_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessFrame(frame):\n",
    "    YOLO_INPUT_WIDTH = 416\n",
    "    YOLO_INPUT_HEIGHT = 416\n",
    "    # Resize the frame to match the input size expected by the YOLO model\n",
    "    resized_frame = cv2.resize(frame, (YOLO_INPUT_WIDTH, YOLO_INPUT_HEIGHT))\n",
    "\n",
    "    # Normalize pixel values to be in the range [0, 1]\n",
    "    normalized_frame = resized_frame / 255.0\n",
    "\n",
    "    # Expand dimensions to add batch dimension\n",
    "    input_frame = np.expand_dims(normalized_frame, axis=0)\n",
    "\n",
    "    return input_frame    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "projectPath=os.getcwd()\n",
    "#modelo_cargado = load_model('VGGModel.keras')\n",
    "modelo_cargado = load_model(os.path.join(projectPath,'Saved_model_vgg'))\n",
    "YoloModel = load_model('YoloModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-08 09:51:12.422 python[5748:249301] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 258ms/step\n",
      "[[0.71483433]]\n",
      "1/1 [==============================] - 0s 295ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "[[0.4847006]]\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "[[0.47060916]]\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "[[0.5198218]]\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "[[0.43321314]]\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "[[0.40692422]]\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "[[0.43521482]]\n",
      "1/1 [==============================] - 0s 275ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "[[0.44485804]]\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "[[0.46086264]]\n",
      "1/1 [==============================] - 0s 287ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "[[0.44649208]]\n",
      "1/1 [==============================] - 0s 284ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "[[0.43192425]]\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "[[0.50981086]]\n",
      "1/1 [==============================] - 0s 273ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "[[0.52744895]]\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "[[0.57972217]]\n",
      "1/1 [==============================] - 0s 274ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "[[0.50340515]]\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "[[0.42782634]]\n",
      "1/1 [==============================] - 0s 270ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "[[0.5220216]]\n",
      "1/1 [==============================] - 0s 274ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "[[0.5364653]]\n",
      "1/1 [==============================] - 0s 272ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "[[0.5316587]]\n",
      "1/1 [==============================] - 0s 273ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "[[0.5389854]]\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "[[0.48444596]]\n",
      "1/1 [==============================] - 0s 273ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "[[0.48090154]]\n",
      "1/1 [==============================] - 0s 270ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "[[0.5052455]]\n",
      "1/1 [==============================] - 0s 273ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "[[0.38513172]]\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "[[0.4479109]]\n",
      "1/1 [==============================] - 0s 274ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "[[0.40359777]]\n",
      "1/1 [==============================] - 0s 268ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "[[0.4909784]]\n",
      "1/1 [==============================] - 0s 273ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "[[0.4667524]]\n",
      "1/1 [==============================] - 0s 269ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "[[0.5210157]]\n",
      "1/1 [==============================] - 0s 275ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "[[0.48775986]]\n",
      "1/1 [==============================] - 0s 274ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "[[0.51241475]]\n",
      "1/1 [==============================] - 0s 271ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "[[0.48091495]]\n",
      "1/1 [==============================] - 0s 269ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "[[0.53586715]]\n",
      "1/1 [==============================] - 0s 273ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "[[0.4806508]]\n",
      "1/1 [==============================] - 0s 272ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "[[0.45762974]]\n",
      "1/1 [==============================] - 0s 285ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "[[0.5117499]]\n",
      "1/1 [==============================] - 0s 272ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "[[0.4642168]]\n",
      "1/1 [==============================] - 0s 272ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "[[0.4791865]]\n",
      "1/1 [==============================] - 0s 272ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "[[0.5530231]]\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "[[0.63025904]]\n",
      "1/1 [==============================] - 0s 273ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "[[0.57688576]]\n",
      "1/1 [==============================] - 0s 271ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "[[0.6024304]]\n",
      "1/1 [==============================] - 0s 274ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "[[0.58514184]]\n",
      "1/1 [==============================] - 0s 272ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "[[0.6004601]]\n",
      "1/1 [==============================] - 0s 299ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "[[0.5938042]]\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "[[0.9249848]]\n",
      "1/1 [==============================] - 0s 304ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "[[0.6689504]]\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "[[0.71483433]]\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "[[0.7272364]]\n",
      "1/1 [==============================] - 0s 283ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "[[0.5969177]]\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "[[0.71483433]]\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "[[0.71483433]]\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "[[0.71483433]]\n",
      "1/1 [==============================] - 0s 285ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "[[0.71483433]]\n",
      "1/1 [==============================] - 0s 283ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "[[0.71483433]]\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "[[0.71483433]]\n",
      "1/1 [==============================] - 0s 280ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "[[0.71483433]]\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "[[0.71483433]]\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "[[0.71483433]]\n",
      "1/1 [==============================] - 0s 285ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "[[0.71483433]]\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "[[0.71483433]]\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "[[0.71483433]]\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "[[0.71483433]]\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "[[0.628651]]\n",
      "1/1 [==============================] - 0s 284ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "[[0.6853669]]\n",
      "1/1 [==============================] - 0s 286ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "[[0.6408761]]\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "[[0.65062094]]\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "OpenCV(4.8.1) /Users/runner/work/opencv-python/opencv-python/opencv/modules/imgproc/src/resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Open the webcam (default camera index is usually 0)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    # Check if the webcam is opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        exit()\n",
    "\n",
    "    while True:\n",
    "        # Read a frame from the webcam\n",
    "        ret, frame = cap.read()\n",
    "        #frame = cv2.imread('despierta.jpg')\n",
    "        #frame = cv2.imread('con_sueno.jpg')\n",
    "        preprocessedFrame=preprocessFrame(frame)\n",
    "        facePrep = YoloModel.predict(preprocessedFrame)\n",
    "        try:\n",
    "            extractedFace, b_boxes = extractFace(facePrep,frame)\n",
    "            pred = prediccion(modelo_cargado, extractedFace, 0.30)\n",
    "            draw_boxes(frame, b_boxes, pred)\n",
    "            #print(pred)\n",
    "        except UnboundLocalError as e:\n",
    "            # Handle the exception\n",
    "            print(f\"No se encuentra el rostro...\")\n",
    "        # Additional handling or logging if needed\n",
    "        # Display the frame\n",
    "        cv2.imshow('Webcam', frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the webcam and close the window\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "except Exception as e:\n",
    "    # Release the webcam and close the window\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"{e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vpc2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
